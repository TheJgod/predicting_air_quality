{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3fccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROPHET PIPELINE - ENHANCED SEASONALITY & SMART IMPUTATION\n",
      "======================================================================\n",
      "\n",
      "Adding features...\n",
      "  ✓ Temporal & COVID features added\n",
      "\n",
      "✓ Filtered to 2021-01-01 onwards: 32207 samples\n",
      "\n",
      "Removing outliers (beyond 1.0th and 99.0th percentile):\n",
      "  valeur_NO2: removed 581 outliers (range: 4.60 - 73.09)\n",
      "  valeur_CO: removed 390 outliers (range: 0.10 - 0.60)\n",
      "  valeur_O3: removed 639 outliers (range: 0.60 - 120.63)\n",
      "  valeur_PM10: removed 516 outliers (range: 3.20 - 58.92)\n",
      "  valeur_PM25: removed 623 outliers (range: 1.90 - 40.91)\n",
      "\n",
      "Interpolating small gaps (<6 hours):\n",
      "  valeur_NO2: filled 868 values\n",
      "  valeur_CO: filled 595 values\n",
      "  valeur_O3: filled 813 values\n",
      "  valeur_PM10: filled 4882 values\n",
      "  valeur_PM25: filled 916 values\n",
      "\n",
      "Imputing large gaps with XGBoost:\n",
      "  valeur_NO2: imputed 2213 values\n",
      "  valeur_CO: imputed 11722 values\n",
      "  valeur_O3: imputed 61 values\n",
      "  valeur_PM10: imputed 1455 values\n",
      "  valeur_PM25: imputed 126 values\n",
      "\n",
      "======================================================================\n",
      "DATA SUMMARY\n",
      "======================================================================\n",
      "Training period: 2021-01-01 00:00:00 to 2024-09-03 22:00:00\n",
      "Total samples: 32207\n",
      "\n",
      "Missing values after imputation:\n",
      "  valeur_NO2: 0 (0.00%)\n",
      "  valeur_CO: 0 (0.00%)\n",
      "  valeur_O3: 0 (0.00%)\n",
      "  valeur_PM10: 0 (0.00%)\n",
      "  valeur_PM25: 0 (0.00%)\n",
      "\n",
      "Training Prophet models with enhanced seasonality:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ valeur_NO2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:01:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ valeur_CO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:01:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ valeur_O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:01:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:12 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ valeur_PM10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:02:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ valeur_PM25\n",
      "\n",
      "Generating predictions:\n",
      "  ✓ valeur_NO2: mean=16.65, median=16.68\n",
      "  ✓ valeur_CO: mean=0.17, median=0.17\n",
      "  ✓ valeur_O3: mean=53.66, median=52.50\n",
      "  ✓ valeur_PM10: mean=12.91, median=13.07\n",
      "  ✓ valeur_PM25: mean=6.08, median=6.13\n",
      "\n",
      "✓ Saved: submission_prophet_enhanced.csv\n",
      "\n",
      "======================================================================\n",
      "COMPARISON WITH FRIEND'S SUBMISSION\n",
      "======================================================================\n",
      "\n",
      "Pollutant       Prophet      Friend       Diff       % Diff\n",
      "----------------------------------------------------------------------\n",
      "valeur_NO2      16.65        20.19        -3.54       -17.5%\n",
      "valeur_CO       0.17         0.18         -0.01        -3.3%\n",
      "valeur_O3       53.66        42.71        10.95        25.6%\n",
      "valeur_PM10     12.91        14.16        -1.25        -8.8%\n",
      "valeur_PM25     6.08         8.88         -2.80       -31.5%\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key improvements:\n",
      "  • Started from 2021 (removed COVID anomaly)\n",
      "  • Removed top/bottom 1% outliers\n",
      "  • Smart imputation: interpolation + XGBoost\n",
      "  • Enhanced seasonality (20 Fourier terms for O3)\n",
      "  • Kept COVID feature for context\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from prophet import Prophet\n",
    "from typing import Dict, List, Tuple\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'start_date': '2021-01-01',\n",
    "    'outlier_percentile': 1.0,  # Remove top/bottom 1%\n",
    "    'small_gap_hours': 6,\n",
    "    'pollutants': ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add comprehensive temporal features.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['datetime'] = pd.to_datetime(df['id'], format='%Y-%m-%d %H')\n",
    "    \n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    df['dayofyear'] = df['datetime'].dt.dayofyear\n",
    "    df['week'] = df['datetime'].dt.isocalendar().week\n",
    "    df['quarter'] = df['datetime'].dt.quarter\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "    \n",
    "    # Binary flags\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    df['is_rush_hour'] = ((df['hour'].between(7, 9)) | (df['hour'].between(17, 19))).astype(int)\n",
    "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "    df['is_business_hours'] = ((df['hour'].between(9, 18)) & (df['dayofweek'] < 5)).astype(int)\n",
    "    \n",
    "    # Holidays\n",
    "    fr_holidays = holidays.France(years=range(2020, 2026))\n",
    "    df['is_holiday'] = df['datetime'].dt.date.isin(fr_holidays).astype(int)\n",
    "    \n",
    "    # Seasonal periods\n",
    "    df['is_summer_vacation'] = df['month'].isin([7, 8]).astype(int)\n",
    "    df['is_winter_vacation'] = (((df['month'] == 12) & (df['day'] >= 20)) | \n",
    "                                 ((df['month'] == 1) & (df['day'] <= 5))).astype(int)\n",
    "    df['is_heating_season'] = ((df['month'] >= 10) | (df['month'] <= 4)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_covid_features(df: pd.DataFrame, covid_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Add COVID stringency index.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    try:\n",
    "        covid_index = pd.read_csv(covid_path)\n",
    "        covid_index['date'] = pd.to_datetime(covid_index['Date'], format='%Y%m%d').dt.date\n",
    "        covid_index = (\n",
    "            covid_index\n",
    "            .query(\"CountryName == 'France'\")\n",
    "            [['date', 'StringencyIndex_Average']]\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        df = df.merge(covid_index, on='date', how='left')\n",
    "        df['StringencyIndex_Average'] = df['StringencyIndex_Average'].fillna(0)\n",
    "    except:\n",
    "        df['StringencyIndex_Average'] = 0\n",
    "    \n",
    "    df = df.drop(columns='date')\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "def filter_post_2021(df: pd.DataFrame, start_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Filter data to start from specified date.\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df[df['datetime'] >= start_date].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, pollutants: List[str], percentile: float) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers beyond specified percentile.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\nRemoving outliers (beyond {percentile}th and {100-percentile}th percentile):\")\n",
    "    for pollutant in pollutants:\n",
    "        original_count = df[pollutant].notna().sum()\n",
    "        \n",
    "        lower = df[pollutant].quantile(percentile / 100)\n",
    "        upper = df[pollutant].quantile(1 - percentile / 100)\n",
    "        \n",
    "        # Set outliers to NaN (will be imputed later)\n",
    "        df.loc[(df[pollutant] < lower) | (df[pollutant] > upper), pollutant] = np.nan\n",
    "        \n",
    "        removed = original_count - df[pollutant].notna().sum()\n",
    "        print(f\"  {pollutant}: removed {removed} outliers (range: {lower:.2f} - {upper:.2f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MISSING VALUE IMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "def identify_gaps(series: pd.Series) -> Tuple[List, List]:\n",
    "    \"\"\"Identify small (<6h) and large gaps.\"\"\"\n",
    "    is_missing = series.isna()\n",
    "    \n",
    "    # Find consecutive missing sequences\n",
    "    gap_starts = []\n",
    "    gap_lengths = []\n",
    "    \n",
    "    in_gap = False\n",
    "    gap_start = None\n",
    "    gap_length = 0\n",
    "    \n",
    "    for idx, missing in enumerate(is_missing):\n",
    "        if missing and not in_gap:\n",
    "            in_gap = True\n",
    "            gap_start = idx\n",
    "            gap_length = 1\n",
    "        elif missing and in_gap:\n",
    "            gap_length += 1\n",
    "        elif not missing and in_gap:\n",
    "            gap_starts.append(gap_start)\n",
    "            gap_lengths.append(gap_length)\n",
    "            in_gap = False\n",
    "            gap_start = None\n",
    "            gap_length = 0\n",
    "    \n",
    "    # Handle case where series ends with gap\n",
    "    if in_gap:\n",
    "        gap_starts.append(gap_start)\n",
    "        gap_lengths.append(gap_length)\n",
    "    \n",
    "    small_gaps = [(s, l) for s, l in zip(gap_starts, gap_lengths) if l <= CONFIG['small_gap_hours']]\n",
    "    large_gaps = [(s, l) for s, l in zip(gap_starts, gap_lengths) if l > CONFIG['small_gap_hours']]\n",
    "    \n",
    "    return small_gaps, large_gaps\n",
    "\n",
    "\n",
    "def interpolate_small_gaps(df: pd.DataFrame, pollutants: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Interpolate small gaps (<6 hours).\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nInterpolating small gaps (<6 hours):\")\n",
    "    for pollutant in pollutants:\n",
    "        before_missing = df[pollutant].isna().sum()\n",
    "        \n",
    "        # Linear interpolation with limit\n",
    "        df[pollutant] = df[pollutant].interpolate(\n",
    "            method='linear', \n",
    "            limit=CONFIG['small_gap_hours'],\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "        after_missing = df[pollutant].isna().sum()\n",
    "        filled = before_missing - after_missing\n",
    "        print(f\"  {pollutant}: filled {filled} values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_imputation_model(df: pd.DataFrame, pollutant: str, \n",
    "                          feature_cols: List[str]) -> xgb.XGBRegressor:\n",
    "    \"\"\"Train XGBoost model for imputing missing values.\"\"\"\n",
    "    # Get non-missing data\n",
    "    train_data = df[df[pollutant].notna()].copy()\n",
    "    \n",
    "    if len(train_data) < 100:\n",
    "        return None\n",
    "    \n",
    "    X = train_data[feature_cols]\n",
    "    y = train_data[pollutant]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def impute_large_gaps(df: pd.DataFrame, pollutants: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Use XGBoost to impute large gaps.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Features for imputation (temporal only, no other pollutants to avoid cascading errors)\n",
    "    imputation_features = [\n",
    "        'hour', 'dayofweek', 'month', 'dayofyear', 'quarter',\n",
    "        'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos', 'dayofyear_sin', 'dayofyear_cos',\n",
    "        'is_weekend', 'is_rush_hour', 'is_night', 'is_business_hours',\n",
    "        'is_holiday', 'is_summer_vacation', 'is_winter_vacation', 'is_heating_season',\n",
    "        'StringencyIndex_Average'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nImputing large gaps with XGBoost:\")\n",
    "    for pollutant in pollutants:\n",
    "        missing_count = df[pollutant].isna().sum()\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            print(f\"  {pollutant}: no missing values\")\n",
    "            continue\n",
    "        \n",
    "        # Train imputation model\n",
    "        model = train_imputation_model(df, pollutant, imputation_features)\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"  {pollutant}: insufficient data, using forward fill\")\n",
    "            df[pollutant] = df[pollutant].ffill().bfill()\n",
    "            continue\n",
    "        \n",
    "        # Predict missing values\n",
    "        missing_mask = df[pollutant].isna()\n",
    "        if missing_mask.sum() > 0:\n",
    "            X_missing = df.loc[missing_mask, imputation_features]\n",
    "            predictions = model.predict(X_missing)\n",
    "            df.loc[missing_mask, pollutant] = np.maximum(0, predictions)\n",
    "        \n",
    "        print(f\"  {pollutant}: imputed {missing_count} values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROPHET MODELING\n",
    "# ============================================================================\n",
    "\n",
    "def train_prophet_model(df: pd.DataFrame, pollutant: str, \n",
    "                       regressors: List[str], seasonality_config: Dict) -> Prophet:\n",
    "    \"\"\"Train Prophet with strong seasonality.\"\"\"\n",
    "    prophet_df = pd.DataFrame({\n",
    "        'ds': df['datetime'],\n",
    "        'y': df[pollutant]\n",
    "    })\n",
    "    \n",
    "    for regressor in regressors:\n",
    "        prophet_df[regressor] = df[regressor].values\n",
    "    \n",
    "    prophet_df = prophet_df.dropna(subset=['y'])\n",
    "    \n",
    "    # Initialize Prophet with enhanced seasonality\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=seasonality_config['yearly'],\n",
    "        weekly_seasonality=seasonality_config['weekly'],\n",
    "        daily_seasonality=seasonality_config['daily'],\n",
    "        seasonality_mode=seasonality_config['mode'],\n",
    "        changepoint_prior_scale=seasonality_config['changepoint_scale']\n",
    "    )\n",
    "    \n",
    "    # Add custom seasonalities with more Fourier terms\n",
    "    if seasonality_config.get('custom_yearly', False):\n",
    "        model.add_seasonality(\n",
    "            name='yearly_strong',\n",
    "            period=365.25,\n",
    "            fourier_order=seasonality_config['yearly_fourier']\n",
    "        )\n",
    "    \n",
    "    if seasonality_config.get('custom_monthly', False):\n",
    "        model.add_seasonality(\n",
    "            name='monthly',\n",
    "            period=30.5,\n",
    "            fourier_order=seasonality_config['monthly_fourier']\n",
    "        )\n",
    "    \n",
    "    # Add regressors\n",
    "    for regressor in regressors:\n",
    "        model.add_regressor(regressor)\n",
    "    \n",
    "    model.fit(prophet_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_all_models(df: pd.DataFrame, pollutants: List[str], \n",
    "                    regressors: List[str]) -> Dict[str, Prophet]:\n",
    "    \"\"\"Train Prophet models for all pollutants.\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Different seasonality configs per pollutant\n",
    "    seasonality_configs = {\n",
    "        'valeur_O3': {  # O3 has strongest seasonality\n",
    "            'yearly': False,  # Use custom instead\n",
    "            'weekly': True,\n",
    "            'daily': True,\n",
    "            'mode': 'multiplicative',\n",
    "            'changepoint_scale': 0.05,\n",
    "            'custom_yearly': True,\n",
    "            'yearly_fourier': 20,  # Strong yearly pattern\n",
    "            'custom_monthly': True,\n",
    "            'monthly_fourier': 8\n",
    "        },\n",
    "        'default': {  # For other pollutants\n",
    "            'yearly': False,\n",
    "            'weekly': True,\n",
    "            'daily': True,\n",
    "            'mode': 'multiplicative',\n",
    "            'changepoint_scale': 0.05,\n",
    "            'custom_yearly': True,\n",
    "            'yearly_fourier': 12,\n",
    "            'custom_monthly': True,\n",
    "            'monthly_fourier': 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTraining Prophet models with enhanced seasonality:\")\n",
    "    for pollutant in pollutants:\n",
    "        config = seasonality_configs.get(pollutant, seasonality_configs['default'])\n",
    "        model = train_prophet_model(df, pollutant, regressors, config)\n",
    "        models[pollutant] = model\n",
    "        print(f\"  ✓ {pollutant}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def generate_predictions(models: Dict[str, Prophet], test: pd.DataFrame,\n",
    "                        pollutants: List[str], regressors: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Generate predictions for test set.\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    print(\"\\nGenerating predictions:\")\n",
    "    for pollutant in pollutants:\n",
    "        future_df = pd.DataFrame({'ds': test['datetime']})\n",
    "        \n",
    "        for regressor in regressors:\n",
    "            future_df[regressor] = test[regressor].values\n",
    "        \n",
    "        forecast = models[pollutant].predict(future_df)\n",
    "        predictions[pollutant] = np.maximum(0, forecast['yhat'].values)\n",
    "        \n",
    "        print(f\"  ✓ {pollutant}: mean={predictions[pollutant].mean():.2f}, \"\n",
    "              f\"median={np.median(predictions[pollutant]):.2f}\")\n",
    "    \n",
    "    submission = test[['id']].copy()\n",
    "    for pollutant in pollutants:\n",
    "        submission[pollutant] = predictions[pollutant]\n",
    "    \n",
    "    return submission\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON & REPORTING\n",
    "# ============================================================================\n",
    "\n",
    "def compare_with_friend(submission: pd.DataFrame, friend_path: str,\n",
    "                       pollutants: List[str]) -> None:\n",
    "    \"\"\"Compare predictions with friend's submission.\"\"\"\n",
    "    friend = pd.read_csv(friend_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON WITH FRIEND'S SUBMISSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n{'Pollutant':<15} {'Prophet':<12} {'Friend':<12} {'Diff':<10} {'% Diff'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for p in pollutants:\n",
    "        prophet_val = submission[p].mean()\n",
    "        friend_val = friend[p].mean()\n",
    "        diff = prophet_val - friend_val\n",
    "        pct_diff = (diff / friend_val * 100) if friend_val != 0 else 0\n",
    "        print(f\"{p:<15} {prophet_val:<12.2f} {friend_val:<12.2f} {diff:<10.2f} {pct_diff:>6.1f}%\")\n",
    "\n",
    "\n",
    "def print_data_summary(train: pd.DataFrame, pollutants: List[str]) -> None:\n",
    "    \"\"\"Print summary of data quality.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training period: {train['datetime'].min()} to {train['datetime'].max()}\")\n",
    "    print(f\"Total samples: {len(train)}\")\n",
    "    print(f\"\\nMissing values after imputation:\")\n",
    "    for p in pollutants:\n",
    "        missing = train[p].isna().sum()\n",
    "        print(f\"  {p}: {missing} ({missing/len(train)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROPHET PIPELINE - ENHANCED SEASONALITY & SMART IMPUTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    pollutants = CONFIG['pollutants']\n",
    "    \n",
    "    # Add features\n",
    "    print(\"\\nAdding features...\")\n",
    "    train = add_temporal_features(train)\n",
    "    test = add_temporal_features(test)\n",
    "    train = add_covid_features(train, 'data/OxCGRT_compact_national_v1.csv')\n",
    "    test = add_covid_features(test, 'data/OxCGRT_compact_national_v1.csv')\n",
    "    print(\"  ✓ Temporal & COVID features added\")\n",
    "    \n",
    "    # Filter to 2021+\n",
    "    train = filter_post_2021(train, CONFIG['start_date'])\n",
    "    print(f\"\\n✓ Filtered to {CONFIG['start_date']} onwards: {len(train)} samples\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    train = remove_outliers(train, pollutants, CONFIG['outlier_percentile'])\n",
    "    \n",
    "    # Impute missing values\n",
    "    train = interpolate_small_gaps(train, pollutants)\n",
    "    train = impute_large_gaps(train, pollutants)\n",
    "    \n",
    "    # Data summary\n",
    "    print_data_summary(train, pollutants)\n",
    "    \n",
    "    # Define regressors\n",
    "    regressors = [\n",
    "        'hour', 'dayofweek', 'month', 'dayofyear', 'quarter',\n",
    "        'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "        'month_sin', 'month_cos', 'dayofyear_sin', 'dayofyear_cos',\n",
    "        'is_weekend', 'is_rush_hour', 'is_night', 'is_business_hours',\n",
    "        'is_holiday', 'is_summer_vacation', 'is_winter_vacation', 'is_heating_season',\n",
    "        'StringencyIndex_Average'\n",
    "    ]\n",
    "    \n",
    "    # Train models\n",
    "    models = train_all_models(train, pollutants, regressors)\n",
    "    \n",
    "    # Generate predictions\n",
    "    submission = generate_predictions(models, test, pollutants, regressors)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission_prophet_enhanced.csv', index=False)\n",
    "    print(\"\\n✓ Saved: submission_prophet_enhanced.csv\")\n",
    "    \n",
    "    # Compare with friend\n",
    "    compare_with_friend(submission, 'prophet_new_predictions (5).csv', pollutants)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nKey improvements:\")\n",
    "    print(\"  • Started from 2021 (removed COVID anomaly)\")\n",
    "    print(\"  • Removed top/bottom 1% outliers\")\n",
    "    print(\"  • Smart imputation: interpolation + XGBoost\")\n",
    "    print(\"  • Enhanced seasonality (20 Fourier terms for O3)\")\n",
    "    print(\"  • Kept COVID feature for context\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5743ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created hybrid submission with friend's O3\n",
      "\n",
      "Pollutant sources:\n",
      "  • valeur_NO2:  Prophet (yours)\n",
      "  • valeur_CO:   Prophet (yours)\n",
      "  • valeur_O3:   Friend's model ✓\n",
      "  • valeur_PM10: Prophet (yours)\n",
      "  • valeur_PM25: Prophet (yours)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both submissions\n",
    "my_submission = pd.read_csv('submission_prophet_enhanced.csv')\n",
    "friend_submission = pd.read_csv('prophet_new_predictions (5).csv')\n",
    "\n",
    "# Replace O3 with friend's O3\n",
    "my_submission['valeur_O3'] = friend_submission['valeur_O3']\n",
    "\n",
    "# Save hybrid submission\n",
    "my_submission.to_csv('submission_prophet_hybrid_o3.csv', index=False)\n",
    "\n",
    "print(\"✓ Created hybrid submission with friend's O3\")\n",
    "print(\"\\nPollutant sources:\")\n",
    "print(\"  • valeur_NO2:  Prophet (yours)\")\n",
    "print(\"  • valeur_CO:   Prophet (yours)\")\n",
    "print(\"  • valeur_O3:   Friend's model ✓\")\n",
    "print(\"  • valeur_PM10: Prophet (yours)\")\n",
    "print(\"  • valeur_PM25: Prophet (yours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROPHET PIPELINE - USING MAXIM'S FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "Original data: Train=40991, Test=504\n",
      "\n",
      "Adding temporal features...\n",
      "  ✓ Temporal features with cyclical encoding\n",
      "\n",
      "Processing weather data...\n",
      "  ✓ Weather data processed and enriched\n",
      "  ✓ Weather merged to training data\n",
      "\n",
      "✓ Filtered to 2021-01-01 onwards: 32207 samples\n",
      "\n",
      "Removing outliers (beyond 1.0th and 99.0th percentile):\n",
      "  valeur_NO2: removed 581 outliers\n",
      "  valeur_CO: removed 390 outliers\n",
      "  valeur_O3: removed 639 outliers\n",
      "  valeur_PM10: removed 516 outliers\n",
      "  valeur_PM25: removed 623 outliers\n",
      "\n",
      "Creating lagged features...\n",
      "  ✓ Lagged and rolling features created\n",
      "\n",
      "Imputing missing values with IterativeImputer + LightGBM...\n",
      "Imputation using 92 features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from prophet import Prophet\n",
    "from typing import Dict, List\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the utility functions (assuming they're in src/features/feature_engineering_maxim.py)\n",
    "# If not, copy them to the top of this file\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'start_date': '2021-01-01',\n",
    "    'outlier_percentile': 1.0,\n",
    "    'pollutants': ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25'],\n",
    "    'weather_file': 'data/paris_meteostat_2020_2025.csv'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def cyclical_encoding(col):\n",
    "    \"\"\"Create sin/cos encoding for cyclical features.\"\"\"\n",
    "    if not isinstance(col, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "    \n",
    "    max_val = col.max()\n",
    "    scaled = (col * 2 * np.pi) / max_val\n",
    "    return np.sin(scaled), np.cos(scaled)\n",
    "\n",
    "\n",
    "def get_features(data: pd.DataFrame, idcol: str = \"id\", encode_cyclical: bool = True):\n",
    "    \"\"\"Add temporal features with cyclical encoding.\"\"\"\n",
    "    fr_holidays = holidays.France()\n",
    "    df = data.copy()\n",
    "    \n",
    "    df[idcol] = pd.to_datetime(df[idcol], format='%Y-%m-%d %H')\n",
    "    df[\"year\"] = df[idcol].dt.year\n",
    "    df[\"month\"] = df[idcol].dt.month\n",
    "    df[\"hour\"] = df[idcol].dt.hour\n",
    "    df[\"weekday\"] = df[idcol].dt.dayofweek\n",
    "    df[\"is_weekend\"] = (df[idcol].dt.dayofweek >= 5).astype(int)\n",
    "    df[\"is_holiday\"] = df[idcol].dt.date.apply(lambda x: x in fr_holidays).astype(int)\n",
    "    \n",
    "    if encode_cyclical:\n",
    "        cyclical_features = [\"month\", \"weekday\", \"hour\"]\n",
    "        for col in cyclical_features:\n",
    "            df[f\"{col}_sin\"], df[f\"{col}_cos\"] = cyclical_encoding(df[col])\n",
    "        df.drop(columns=cyclical_features, inplace=True)\n",
    "    \n",
    "    return df.sort_values(by=idcol, ascending=True)\n",
    "\n",
    "\n",
    "def fill_weather_gaps(df_weather: pd.DataFrame, start=None, end=None) -> pd.DataFrame:\n",
    "    \"\"\"Fill missing rows and interpolate weather data.\"\"\"\n",
    "    df = df_weather.copy()\n",
    "    \n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"]).sort_values(\"time\")\n",
    "    df = df[~df[\"time\"].duplicated(keep=\"first\")]\n",
    "    \n",
    "    if start is None:\n",
    "        start = df[\"time\"].min().floor(\"H\")\n",
    "    else:\n",
    "        start = pd.to_datetime(start).floor(\"H\")\n",
    "    if end is None:\n",
    "        end = df[\"time\"].max().ceil(\"H\")\n",
    "    else:\n",
    "        end = pd.to_datetime(end).ceil(\"H\")\n",
    "    \n",
    "    full_idx = pd.date_range(start, end, freq=\"H\")\n",
    "    df = df.set_index(\"time\").reindex(full_idx)\n",
    "    \n",
    "    cont_cols = [\"temp\", \"dwpt\", \"rhum\", \"wdir\", \"wspd\", \"pres\"]\n",
    "    for col in cont_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "    \n",
    "    if \"prcp\" in df.columns:\n",
    "        df[\"prcp\"] = df[\"prcp\"].fillna(0)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill(limit=3).bfill(limit=3)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"time\"})\n",
    "    \n",
    "    cols_order = [c for c in [\"time\",\"temp\",\"dwpt\",\"rhum\",\"prcp\",\"wdir\",\"wspd\",\"pres\"] if c in df.columns]\n",
    "    df = df[cols_order + [c for c in df.columns if c not in cols_order]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_weather_features(df_weather: pd.DataFrame, tz_local: str = \"Europe/Paris\") -> pd.DataFrame:\n",
    "    \"\"\"Enrich weather data with physical features.\"\"\"\n",
    "    df = df_weather.copy()\n",
    "    df = df.drop_duplicates(subset=[\"time\"]).sort_values(\"time\").reset_index(drop=True)\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    \n",
    "    # Wind features\n",
    "    wdir = df[\"wdir\"].fillna(0)\n",
    "    wdir_rad = np.deg2rad(wdir % 360)\n",
    "    df[\"wind_u\"] = -df[\"wspd\"] * np.sin(wdir_rad)\n",
    "    df[\"wind_v\"] = -df[\"wspd\"] * np.cos(wdir_rad)\n",
    "    \n",
    "    # Humidity physics\n",
    "    es = 0.6108 * np.exp(17.27 * df[\"temp\"] / (df[\"temp\"] + 237.3))\n",
    "    ea = 0.6108 * np.exp(17.27 * df[\"dwpt\"] / (df[\"dwpt\"] + 237.3))\n",
    "    df[\"vpd\"] = (es - ea).clip(lower=0)\n",
    "    df[\"abs_humidity\"] = 216.7 * (ea / (df[\"temp\"] + 273.15))\n",
    "    \n",
    "    # Rain features\n",
    "    df[\"is_wet_hour\"] = (df[\"prcp\"].fillna(0) > 0).astype(int)\n",
    "    df[\"rain_3h_sum\"] = df[\"prcp\"].rolling(3, min_periods=1).sum()\n",
    "    \n",
    "    # Pressure anomaly\n",
    "    df[\"pres_anom_7d\"] = df[\"pres\"] - df[\"pres\"].rolling(24 * 7, min_periods=12).mean()\n",
    "    \n",
    "    # Solar geometry\n",
    "    lat = np.deg2rad(48.8566)\n",
    "    df_local = df.copy()\n",
    "    df_local[\"time_tz\"] = pd.to_datetime(df_local[\"time\"]).dt.tz_localize(\n",
    "        tz_local, ambiguous=\"NaT\", nonexistent=\"shift_forward\"\n",
    "    )\n",
    "    doy = df_local[\"time_tz\"].dt.dayofyear.values\n",
    "    hour = df_local[\"time_tz\"].dt.hour.values + df_local[\"time_tz\"].dt.minute.values / 60.0\n",
    "    \n",
    "    decl = 23.44 * np.pi / 180 * np.sin(2 * np.pi * (284 + doy) / 365)\n",
    "    h_angle = np.pi / 12 * (hour - 12)\n",
    "    solar_elev = np.arcsin(np.sin(lat)*np.sin(decl) + np.cos(lat)*np.cos(decl)*np.cos(h_angle))\n",
    "    df[\"solar_elev_sin\"] = np.sin(np.clip(solar_elev, 0, np.pi/2))\n",
    "    df[\"is_daylight\"] = (solar_elev > 0).astype(int)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill(limit=3)\n",
    "    \n",
    "    keep_cols = [\n",
    "        \"time\", \"temp\", \"rhum\", \"pres\",\n",
    "        \"wind_u\", \"wind_v\",\n",
    "        \"prcp\", \"is_wet_hour\", \"rain_3h_sum\",\n",
    "        \"vpd\", \"abs_humidity\",\n",
    "        \"solar_elev_sin\", \"is_daylight\"\n",
    "    ]\n",
    "    \n",
    "    return df[keep_cols]\n",
    "\n",
    "\n",
    "def merge_weather(main: pd.DataFrame, weather: pd.DataFrame, \n",
    "                 main_col: str = \"id\", weather_col: str = \"time\") -> pd.DataFrame:\n",
    "    \"\"\"Merge weather data with main dataframe.\"\"\"\n",
    "    assert main[main_col].duplicated().sum() == 0, \"Non-unique dates in main\"\n",
    "    assert weather[weather_col].duplicated().sum() == 0, \"Non-unique dates in weather\"\n",
    "    \n",
    "    output = main.merge(weather, left_on=main_col, right_on=weather_col, how=\"left\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_lagged_features(df: pd.DataFrame,\n",
    "                        pollutants: list,\n",
    "                        lags=(1, 3, 6, 12, 24),\n",
    "                        rolls=(3, 6, 12, 24)) -> pd.DataFrame:\n",
    "    \"\"\"Create lagged and rolling features for pollutants.\"\"\"\n",
    "    df = df.copy().sort_values(\"id\")\n",
    "    \n",
    "    for col in pollutants:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        for lag in lags:\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "        \n",
    "        for win in rolls:\n",
    "            roll = df[col].rolling(window=win, min_periods=1)\n",
    "            df[f\"{col}_roll{win}_mean\"] = roll.mean()\n",
    "            df[f\"{col}_roll{win}_std\"] = roll.std()\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_pollutants(df: pd.DataFrame, pollutants: list, \n",
    "                     weather_cols: list, cal_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing pollutant values using IterativeImputer with LightGBM.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for p in pollutants:\n",
    "        df[p + \"_missing\"] = df[p].isna().astype(int)\n",
    "    \n",
    "    lag_roll_cols = []\n",
    "    for p in pollutants:\n",
    "        lag_roll_cols += [c for c in df.columns if c.startswith(p + \"_lag\")]\n",
    "        lag_roll_cols += [c for c in df.columns if c.startswith(p + \"_roll\")]\n",
    "    \n",
    "    cols = pollutants + weather_cols + cal_cols + lag_roll_cols + [p+\"_missing\" for p in pollutants]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    \n",
    "    X = df[cols].astype(float)\n",
    "    \n",
    "    print(f\"Imputation using {len(cols)} features\")\n",
    "    \n",
    "    imp = IterativeImputer(\n",
    "        estimator=LGBMRegressor(n_estimators=50, max_depth=5, random_state=42, verbose=-1),\n",
    "        max_iter=10,\n",
    "        sample_posterior=False,\n",
    "        initial_strategy=\"median\",\n",
    "        skip_complete=True,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    X_imp = imp.fit_transform(X)\n",
    "    X_imp_df = pd.DataFrame(X_imp, columns=X.columns, index=X.index)\n",
    "    \n",
    "    for p in pollutants:\n",
    "        mask = df[p].isna()\n",
    "        df.loc[mask, p] = X_imp_df.loc[mask, p]\n",
    "        df[p] = df[p].clip(lower=0)\n",
    "    \n",
    "    # Drop missing indicator columns\n",
    "    df = df.drop(columns=[p+\"_missing\" for p in pollutants], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "def filter_post_2021(df: pd.DataFrame, start_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Filter data starting from specified date.\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df[df['id'] >= start_date].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, pollutants: List[str], percentile: float) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers beyond specified percentile.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\nRemoving outliers (beyond {percentile}th and {100-percentile}th percentile):\")\n",
    "    for pollutant in pollutants:\n",
    "        if pollutant not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        original_count = df[pollutant].notna().sum()\n",
    "        \n",
    "        lower = df[pollutant].quantile(percentile / 100)\n",
    "        upper = df[pollutant].quantile(1 - percentile / 100)\n",
    "        \n",
    "        df.loc[(df[pollutant] < lower) | (df[pollutant] > upper), pollutant] = np.nan\n",
    "        \n",
    "        removed = original_count - df[pollutant].notna().sum()\n",
    "        print(f\"  {pollutant}: removed {removed} outliers\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROPHET MODELING\n",
    "# ============================================================================\n",
    "\n",
    "def train_prophet_model(df: pd.DataFrame, pollutant: str, \n",
    "                       regressors: List[str], enhanced_o3: bool = False) -> Prophet:\n",
    "    \"\"\"Train Prophet with enhanced seasonality.\"\"\"\n",
    "    prophet_df = pd.DataFrame({\n",
    "        'ds': df['id'],\n",
    "        'y': df[pollutant]\n",
    "    })\n",
    "    \n",
    "    for regressor in regressors:\n",
    "        if regressor in df.columns:\n",
    "            prophet_df[regressor] = df[regressor].values\n",
    "    \n",
    "    prophet_df = prophet_df.dropna(subset=['y'])\n",
    "    \n",
    "    # Enhanced seasonality for O3\n",
    "    if enhanced_o3 and pollutant == 'valeur_O3':\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            seasonality_mode='multiplicative',\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        model.add_seasonality(name='yearly_strong', period=365.25, fourier_order=20)\n",
    "        model.add_seasonality(name='monthly', period=30.5, fourier_order=8)\n",
    "    else:\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            seasonality_mode='multiplicative',\n",
    "            changepoint_prior_scale=0.05\n",
    "        )\n",
    "        model.add_seasonality(name='yearly', period=365.25, fourier_order=12)\n",
    "        model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    \n",
    "    for regressor in regressors:\n",
    "        if regressor in prophet_df.columns:\n",
    "            model.add_regressor(regressor)\n",
    "    \n",
    "    model.fit(prophet_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_all_models(df: pd.DataFrame, pollutants: List[str], \n",
    "                    regressors: List[str]) -> Dict[str, Prophet]:\n",
    "    \"\"\"Train Prophet models for all pollutants.\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    print(\"\\nTraining Prophet models:\")\n",
    "    for pollutant in pollutants:\n",
    "        enhanced = (pollutant == 'valeur_O3')\n",
    "        model = train_prophet_model(df, pollutant, regressors, enhanced_o3=enhanced)\n",
    "        models[pollutant] = model\n",
    "        print(f\"  ✓ {pollutant}\" + (\" (enhanced seasonality)\" if enhanced else \"\"))\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def generate_predictions(models: Dict[str, Prophet], test: pd.DataFrame,\n",
    "                        pollutants: List[str], regressors: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Generate predictions for test set.\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    print(\"\\nGenerating predictions:\")\n",
    "    for pollutant in pollutants:\n",
    "        future_df = pd.DataFrame({'ds': test['id']})\n",
    "        \n",
    "        for regressor in regressors:\n",
    "            if regressor in test.columns:\n",
    "                future_df[regressor] = test[regressor].values\n",
    "        \n",
    "        forecast = models[pollutant].predict(future_df)\n",
    "        predictions[pollutant] = np.maximum(0, forecast['yhat'].values)\n",
    "        \n",
    "        print(f\"  ✓ {pollutant}: mean={predictions[pollutant].mean():.2f}\")\n",
    "    \n",
    "    submission = test[['id']].copy()\n",
    "    for pollutant in pollutants:\n",
    "        submission[pollutant] = predictions[pollutant]\n",
    "    \n",
    "    return submission\n",
    "\n",
    "\n",
    "def compare_with_friend(submission: pd.DataFrame, friend_path: str,\n",
    "                       pollutants: List[str]) -> None:\n",
    "    \"\"\"Compare predictions with friend's submission.\"\"\"\n",
    "    friend = pd.read_csv(friend_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON WITH FRIEND'S SUBMISSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n{'Pollutant':<15} {'Prophet':<12} {'Friend':<12} {'Diff':<10} {'% Diff'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for p in pollutants:\n",
    "        prophet_val = submission[p].mean()\n",
    "        friend_val = friend[p].mean()\n",
    "        diff = prophet_val - friend_val\n",
    "        pct_diff = (diff / friend_val * 100) if friend_val != 0 else 0\n",
    "        print(f\"{p:<15} {prophet_val:<12.2f} {friend_val:<12.2f} {diff:<10.2f} {pct_diff:>6.1f}%\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROPHET PIPELINE - USING MAXIM'S FEATURE ENGINEERING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pollutants = CONFIG['pollutants']\n",
    "    \n",
    "    # Load data\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    \n",
    "    print(f\"\\nOriginal data: Train={len(train)}, Test={len(test)}\")\n",
    "    \n",
    "    # Add temporal features\n",
    "    print(\"\\nAdding temporal features...\")\n",
    "    train = get_features(train, idcol='id', encode_cyclical=True)\n",
    "    test = get_features(test, idcol='id', encode_cyclical=True)\n",
    "    print(\"  ✓ Temporal features with cyclical encoding\")\n",
    "    \n",
    "    # Load and process weather\n",
    "    print(\"\\nProcessing weather data...\")\n",
    "    weather_raw = pd.read_csv(CONFIG['weather_file'])\n",
    "    weather_raw = weather_raw.rename(columns={'Unnamed: 0': 'time'})\n",
    "    \n",
    "    # Fill gaps and enrich\n",
    "    weather = fill_weather_gaps(weather_raw)\n",
    "    weather = enrich_weather_features(weather)\n",
    "    print(\"  ✓ Weather data processed and enriched\")\n",
    "    \n",
    "    # Merge weather (only for training - test won't use weather for Prophet regressors)\n",
    "    train = merge_weather(train, weather, main_col='id', weather_col='time')\n",
    "    print(\"  ✓ Weather merged to training data\")\n",
    "    \n",
    "    # Filter to 2021+\n",
    "    train = filter_post_2021(train, CONFIG['start_date'])\n",
    "    print(f\"\\n✓ Filtered to {CONFIG['start_date']} onwards: {len(train)} samples\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    train = remove_outliers(train, pollutants, CONFIG['outlier_percentile'])\n",
    "    \n",
    "    # Create lagged features\n",
    "    print(\"\\nCreating lagged features...\")\n",
    "    train = make_lagged_features(train, pollutants, lags=(1, 3, 6, 12, 24), rolls=(3, 6, 12, 24))\n",
    "    print(\"  ✓ Lagged and rolling features created\")\n",
    "    \n",
    "    # Impute missing values\n",
    "    print(\"\\nImputing missing values with IterativeImputer + LightGBM...\")\n",
    "    weather_cols = ['temp', 'rhum', 'pres', 'wind_u', 'wind_v', 'prcp', \n",
    "                   'vpd', 'abs_humidity', 'solar_elev_sin']\n",
    "    cal_cols = ['month_sin', 'month_cos', 'weekday_sin', 'weekday_cos', \n",
    "               'hour_sin', 'hour_cos', 'is_weekend', 'is_holiday']\n",
    "    \n",
    "    train = impute_pollutants(train, pollutants, weather_cols, cal_cols)\n",
    "    \n",
    "    print(\"\\nMissing values after imputation:\")\n",
    "    for p in pollutants:\n",
    "        missing = train[p].isna().sum()\n",
    "        print(f\"  {p}: {missing}\")\n",
    "    \n",
    "    # Define regressors (only temporal - no weather/lags for Prophet)\n",
    "    regressors = [\n",
    "        'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos',\n",
    "        'hour_sin', 'hour_cos', 'is_weekend', 'is_holiday', 'year'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nUsing {len(regressors)} regressors for Prophet\")\n",
    "    \n",
    "    # Train models\n",
    "    models = train_all_models(train, pollutants, regressors)\n",
    "    \n",
    "    # Generate predictions\n",
    "    submission = generate_predictions(models, test, pollutants, regressors)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission_prophet_maxim.csv', index=False)\n",
    "    print(\"\\n✓ Saved: submission_prophet_maxim.csv\")\n",
    "    \n",
    "    # Compare with friend\n",
    "    compare_with_friend(submission, 'prophet_new_predictions (5).csv', pollutants)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nKey features:\")\n",
    "    print(\"  • Maxim's feature engineering functions\")\n",
    "    print(\"  • Weather from Meteostat with physical features\")\n",
    "    print(\"  • IterativeImputer with LightGBM\")\n",
    "    print(\"  • Enhanced seasonality (20 Fourier for O3)\")\n",
    "    print(\"  • Started from 2021, removed outliers\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04791b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
